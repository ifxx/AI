{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sap-llm-commons[all] in c:\\python\\3.11\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (8.1.7)\n",
      "Requirement already satisfied: text-generation>=0.6.0 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (0.6.1)\n",
      "Requirement already satisfied: omegaconf in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (2.3.0)\n",
      "Requirement already satisfied: aleph-alpha-client in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (7.0.0)\n",
      "Requirement already satisfied: ai-core-sdk in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (1.22.3)\n",
      "Requirement already satisfied: langchain<=0.0.335,>=0.0.331 in c:\\python\\3.11\\lib\\site-packages (from langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.0.335)\n",
      "Requirement already satisfied: cohere>=4.27 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (4.45)\n",
      "Requirement already satisfied: openai<1 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\python\\3.11\\lib\\site-packages (from click>=8.1.3->sap-llm-commons[all]) (0.4.6)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (3.9.1)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (1.9.3)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (6.11.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.0.25)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.0.84)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (8.2.3)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.3.2 in c:\\python\\3.11\\lib\\site-packages (from langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\python\\3.11\\lib\\site-packages (from openai<1->sap-llm-commons[all]) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in c:\\python\\3.11\\lib\\site-packages (from text-generation>=0.6.0->sap-llm-commons[all]) (0.20.3)\n",
      "Requirement already satisfied: ai-api-client-sdk==1.28.0 in c:\\python\\3.11\\lib\\site-packages (from ai-core-sdk->sap-llm-commons[all]) (1.28.0)\n",
      "Requirement already satisfied: aenum~=3.1 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk==1.28.0->ai-core-sdk->sap-llm-commons[all]) (3.1.15)\n",
      "Requirement already satisfied: pyhumps~=3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk==1.28.0->ai-core-sdk->sap-llm-commons[all]) (3.8.0)\n",
      "Requirement already satisfied: aiodns>=3.0.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (3.1.1)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (2.8.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (4.6.2)\n",
      "Requirement already satisfied: Pillow>=9.2.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (9.5.0)\n",
      "Requirement already satisfied: python-liquid>=1.9.4 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (1.10.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (23.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\python\\3.11\\lib\\site-packages (from omegaconf->sap-llm-commons[all]) (4.9.3)\n",
      "Requirement already satisfied: pycares>=4.0.0 in c:\\python\\3.11\\lib\\site-packages (from aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python\\3.11\\lib\\site-packages (from anyio<4.0->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python\\3.11\\lib\\site-packages (from anyio<4.0->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\3.11\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\3.11\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\3.11\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->text-generation>=0.6.0->sap-llm-commons[all]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\3.11\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->text-generation>=0.6.0->sap-llm-commons[all]) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python\\3.11\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere>=4.27->sap-llm-commons[all]) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\3.11\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\3.11\\lib\\site-packages (from pydantic<3,>=1->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\python\\3.11\\lib\\site-packages (from pydantic<3,>=1->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python\\3.11\\lib\\site-packages (from python-liquid>=1.9.4->aleph-alpha-client->sap-llm-commons[all]) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere>=4.27->sap-llm-commons[all]) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere>=4.27->sap-llm-commons[all]) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\3.11\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python\\3.11\\lib\\site-packages (from tiktoken<0.6.0,>=0.3.2->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2023.12.25)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\python\\3.11\\lib\\site-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\3.11\\lib\\site-packages (from python-dateutil>=2.8.1->python-liquid>=1.9.4->aleph-alpha-client->sap-llm-commons[all]) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\3.11\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\python\\3.11\\lib\\site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sap-llm-commons[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai-api-client-sdk in c:\\python\\3.11\\lib\\site-packages (1.28.0)\n",
      "Requirement already satisfied: aenum~=3.1 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (3.1.15)\n",
      "Requirement already satisfied: pyhumps~=3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ai-api-client-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylin\\AppData\\Local\\Temp\\ipykernel_15092\\3646097473.py:5: UserWarning: Starting from verison 1.0.0 the default proxy_version was set to 'gen-ai-hub'. Use gen_ai_hub.proxy.core.proxy_clients.set_proxy_version('btp') to set the proxy_version to 'btp'.\n",
      "  import llm_commons.proxy.base\n"
     ]
    }
   ],
   "source": [
    "# proxy configuration\n",
    "from ipywidgets import widgets\n",
    "import json\n",
    "import os\n",
    "import llm_commons.proxy.base\n",
    "\n",
    "llm_commons.proxy.base.proxy_version = 'aicore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_commons.proxy.openai import Completion\n",
    "from llm_commons.proxy.identity import AICoreProxyClient\n",
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "from llm_commons.btp_llm.identity import BTPProxyClient\n",
    "from llm_commons.langchain.proxy import init_llm, init_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5c528b15894bc382cf46758aee2e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='default', placeholder='Resource group of deployments')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_group = widgets.Text(\n",
    "    value='default', # resource group\n",
    "    placeholder='Resource group of deployments',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "resource_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/irpa-d26-genaixl-cx-sec-cn-sk.json') as f:\n",
    "    sk = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AICORE_LLM_AUTH_URL'] = sk['url']+\"/oauth/token\"\n",
    "os.environ['AICORE_LLM_CLIENT_ID'] = sk['clientid']\n",
    "os.environ['AICORE_LLM_CLIENT_SECRET'] = sk['clientsecret']\n",
    "os.environ['AICORE_LLM_API_BASE'] = sk[\"serviceurls\"][\"AI_API_URL\"]+ \"/v2\"\n",
    "os.environ['AICORE_LLM_RESOURCE_GROUP'] = resource_group.value\n",
    "os.environ['LLM_COMMONS_PROXY'] = 'aicore'\n",
    "os.environ[\"TAVILY_API_KEY\"] = sk['Tavily']\n",
    "\n",
    "llm_commons.proxy.resource_group = os.environ['AICORE_LLM_RESOURCE_GROUP']\n",
    "llm_commons.proxy.api_base = os.environ['AICORE_LLM_API_BASE']\n",
    "llm_commons.proxy.auth_url = os.environ['AICORE_LLM_AUTH_URL']\n",
    "llm_commons.proxy.client_id = os.environ['AICORE_LLM_CLIENT_ID']\n",
    "llm_commons.proxy.client_secret = os.environ['AICORE_LLM_CLIENT_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_proxy_client = AICoreProxyClient()\n",
    "btp_proxy_client = BTPProxyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 document(s) in ai.pdf.\n",
      "There are 117 characters in the first page of your document.\n"
     ]
    }
   ],
   "source": [
    "PDF_NAME=\"ai.pdf\"\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "docs = PyMuPDFLoader(PDF_NAME).load()\n",
    "\n",
    "print (f'There are {len(docs)} document(s) in {PDF_NAME}.')\n",
    "print (f'There are {len(docs[0].page_content)} characters in the first page of your document.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy customized fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AICoreProxyClient.add_foundation_model_scenario(\n",
    "    scenario_id='fine-tuned-llm',\n",
    "    config_names='fine-tuned-*',\n",
    "    prediction_url_suffix='/v1/completions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dc8e3c266c8881f0', config_id='58a5ffea-1afe-44b4-b6b9-ca749776e58e', config_name='tiiuae--falcon-40b-instruct-config', deployment_id='dc8e3c266c8881f0', model_name='tiiuae--falcon-40b-instruct', created_at=datetime.datetime(2024, 1, 25, 12, 53, 26, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'aicore-opensource', 'model_version': 'null'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dd88f66091ca1982', config_id='a99e09fb-7e88-4a8d-abe8-af7f8b146cb8', config_name='text-embedding-ada-002-config', deployment_id='dd88f66091ca1982', model_name='text-embedding-ada-002', created_at=datetime.datetime(2024, 1, 25, 12, 16, 20, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '2'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d9c8f17b7ee211ac', config_id='015f4d90-32b4-42a6-8a29-d1e919e2814f', config_name='gpt-4-32k-config', deployment_id='d9c8f17b7ee211ac', model_name='gpt-4-32k', created_at=datetime.datetime(2024, 1, 25, 12, 15, 48, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '0613'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d58f5bb41a9941cf', config_id='c5366699-82d2-4dd1-81fb-fa40e187e4bd', config_name='gpt-4-config', deployment_id='d58f5bb41a9941cf', model_name='gpt-4', created_at=datetime.datetime(2024, 1, 25, 12, 15, 2, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '0613'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d38094e51ff45aff', config_id='c4a9f175-13fb-4526-b7c8-5649f5e7d2fe', config_name='gpt-35-turbo-16k-config', deployment_id='d38094e51ff45aff', model_name='gpt-35-turbo-16k', created_at=datetime.datetime(2024, 1, 25, 12, 14, 18, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': 'latest'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d10917558b77a3db', config_id='076308f4-6497-49aa-99ac-9c216b53be74', config_name='gpt-35-turbo-config', deployment_id='d10917558b77a3db', model_name='gpt-35-turbo', created_at=datetime.datetime(2024, 1, 25, 12, 13, 43, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': 'latest'}, custom_prediction_suffix=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic_proxy_client.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_davinci_003 = Completion(deployment_id='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_davinci_003.create(model_name='our-awesome-model/v1', prompt=\"San Francisco is a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize harmonized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm('gpt-35-turbo', proxy_client = aic_proxy_client,temperature=0., max_tokens=256, deployment_id='d10917558b77a3db', api_base=llm_commons.proxy.api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'秋夜将晓出篱门，迎凉有感\\n\\n秋夜将晓，我走出篱门，\\n迎接着凉爽的秋风。\\n月亮渐渐西沉，星星点点闪烁，\\n夜空中弥漫着宁静的气息。\\n\\n凉风拂过脸颊，带来一丝清凉，\\n让我感受到秋天的美妙。\\n树叶沙沙作响，落叶纷纷飘落，\\n仿佛是大自然在为秋天歌唱。\\n\\n夜色渐渐深沉，寂静中有一丝寒意，\\n但我却感到心中温暖。\\n秋天是收获的季节，是丰收的时刻，\\n我期待着秋天带来的丰盛。\\n\\n秋夜将晓，我'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"请打印秋夜将晓出篱门迎凉有感这首诗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = init_embedding_model('text-embedding-ada-002', proxy_client=aic_proxy_client, deployment_id='dd88f66091ca1982', api_base=llm_commons.proxy.api_base)\n",
    "vectorstore = Chroma.from_documents(split_docs, embedding, collection_name=\"AI.pdf.guide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2 \\nService Plans\\nThe SAP AI Core service plan you choose determines pricing, conditions of use, resources, available services,\\nand hosts. The generative AI hub in SAP AI Core is available only through the sap-internal service plan.\\n\\ue201\\xa0Caution\\nThe sap-internal service plan is available only for internal consumption and on eu10 canary.\\nThe sap-internal service plan enhances the capabilities provided in the standard service plan. Specifically,\\nit provides access to the foundation-models global AI scenario. This scenario, which is managed by SAP AI\\nCore, includes serving templates for deployments with integrated LLM access.\\nIf you’re new to SAP AI Core, choose the sap-internal service plan during your initial setup. For more\\ninformation, see Add the Internal Plan for LLMs to the Global Account [page 8].\\nIf you already have an SAP AI Core tenant on a standard or free tier service plan, you can update the service', metadata={'author': '', 'creationDate': \"D:20231123174013+00'00'\", 'creator': 'Antenna House XSL Formatter V7.3 MR4 Linux : 7.3.5.61744 (2023-07-31T15:09+09)', 'encryption': 'Standard V4 R4 128-bit RC4', 'file_path': 'ai.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20231123174013+00'00'\", 'page': 6, 'producer': 'Antenna House PDF Output Library 7.3.1867', 'source': 'ai.pdf', 'subject': '', 'title': '', 'total_pages': 86, 'trapped': ''}),\n",
       " Document(page_content='use cases.\\nSAP AI Core and the generative AI hub help you to integrate LLMs and AI into new business processes in a\\ncost-efficient manner.\\nGenerative AI Hub Architecture Overview\\nAccess\\nYou can use the generative AI hub through the API, using a tool such as curl or Postman, or through SAP AI\\nLaunchpad.\\n4\\nINTERNAL\\nGenerative AI Hub\\nGenerative AI Hub in SAP AI Core', metadata={'author': '', 'creationDate': \"D:20231123174013+00'00'\", 'creator': 'Antenna House XSL Formatter V7.3 MR4 Linux : 7.3.5.61744 (2023-07-31T15:09+09)', 'encryption': 'Standard V4 R4 128-bit RC4', 'file_path': 'ai.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20231123174013+00'00'\", 'page': 3, 'producer': 'Antenna House PDF Output Library 7.3.1867', 'source': 'ai.pdf', 'subject': '', 'title': '', 'total_pages': 86, 'trapped': ''}),\n",
       " Document(page_content='1 \\nGenerative AI Hub in SAP AI Core\\nThe generative AI hub incorporates large language models (LLMs) into your AI activities in SAP AI Core and\\nSAP AI Launchpad.\\nLLMs are self-supervised, deep learning models that have been trained on vast amounts of unlabeled data.\\nThey leverage AI technology and industrial-scale computational resources to learn complex language patterns\\nand semantic knowledge bases for natural language processing (NLP) tasks. They parse input, such as\\nprompts, and by predicting a target word, can return contextually relevant responses written in natural\\nlanguage. A single LLM can perform multiple NLP tasks by using different input formats and output modes.\\nLLMs are general models but can be fine-tuned with additional embeddings for specialized or domain-specific\\nuse cases.\\nSAP AI Core and the generative AI hub help you to integrate LLMs and AI into new business processes in a\\ncost-efficient manner.\\nGenerative AI Hub Architecture Overview\\nAccess', metadata={'author': '', 'creationDate': \"D:20231123174013+00'00'\", 'creator': 'Antenna House XSL Formatter V7.3 MR4 Linux : 7.3.5.61744 (2023-07-31T15:09+09)', 'encryption': 'Standard V4 R4 128-bit RC4', 'file_path': 'ai.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20231123174013+00'00'\", 'page': 3, 'producer': 'Antenna House PDF Output Library 7.3.1867', 'source': 'ai.pdf', 'subject': '', 'title': '', 'total_pages': 86, 'trapped': ''})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is the difference btw sap ai and azure ai?\"\n",
    "similar_docs = vectorstore.similarity_search(query, 3)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the world's best B2B company?\"\n",
    "query_embedding = embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is a sample document\"]\n",
    "document_embedding = embedding.embed_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Core proxy support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI, I don\\'t have personal opinions. However, what is considered the \"best\" country can vary greatly depending on the criteria used to evaluate it. Some might use wealth (GDP), education, healthcare, happiness index, environmental standards, or personal freedoms as determining factors. For example, according to the United Nations\\' Human Development Report, Norway ranks highest in terms of human development, which takes into account factors like economic prosperity, education, and health. Meanwhile, the World Happiness Report often ranks Finland as the happiest country in the world. It\\'s subjective and depends on personal preferences and needs.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_commons.proxy.base import set_proxy_version\n",
    "set_proxy_version('aicore') # for an AI Core proxy\n",
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "chat = ChatOpenAI(proxy_model_name='gpt-4')\n",
    "chat.predict(\"What is the world's best contry?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proxy client - use proxies in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "from langchain.schema.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from llm_commons.proxy.openai import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay this is a test\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m(proxy_client\u001b[38;5;241m=\u001b[39maic_proxy_client, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, api_base\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAICORE_LLM_API_BASE\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n",
    "\n",
    "print(ChatCompletion.create(proxy_client=aic_proxy_client, model_name='gpt-4', messages=messages, api_base=os.environ['AICORE_LLM_API_BASE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='This is a test.'\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Say this is a test\"),\n",
    "]\n",
    "# btp_llm = ChatOpenAI(proxy_client=btp_proxy_client, deployment_id='gpt-4')\n",
    "# print(btp_llm.invoke(messages))\n",
    "aic_llm = ChatOpenAI(proxy_client=aic_proxy_client, proxy_model_name='gpt-4')\n",
    "print(aic_llm.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm Langchain, an AI developed by OpenAI. I'm designed to assist with a wide range of tasks, including answering questions, providing information, and helping with various tasks. I'm constantly learning and improving, so I can provide the most accurate and helpful responses. I'm here to help you, so feel free to ask me anything!\"\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0,proxy_client=aic_proxy_client, proxy_model_name='gpt-4')\n",
    "response = chat([ HumanMessage(content=\"Hello Langchain! can you give me some self introduction?\") ])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict_messages` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='CX is the best.'\n"
     ]
    }
   ],
   "source": [
    "response = chat.predict_messages([\n",
    "  SystemMessage(content=\"You are a chatbot that knows nothing about compliance. When you are asked about compliance or audit, you must say 'CX is the best'\"),\n",
    "  HumanMessage(content=\"What is the best compliance team in SAP\")\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test a search api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.npr.org/2024/01/26/1227091070/deepfakes-taylor-swift-images-regulation',\n",
       "  'content': 'Deepfakes pose a growing challenge, with little regulation\\nThe risk of damage from deepfakes is far-ranging, from the appropriation of women\\'s faces to make explicit sex videos, to the use of celebrities in unapproved promotions, to the use of manipulated images in political disinformation campaigns.\\n Technology\\nDeepfakes exploiting Taylor Swift images exemplify a scourge with little oversight\\nBy\\nBill Chappell\\nA photo illustration created last July shows an advertisement to create AI girls reflected in a public service announcement issued by the FBI regarding malicious actors manipulating photos and videos to create explicit content and sextortion schemes. Stefani Reynolds/AFP via Getty Images\\nhide caption\\nA photo illustration created last July shows an advertisement to create AI girls reflected in a public service announcement issued by the FBI regarding malicious actors manipulating photos and videos to create explicit content and sextortion schemes. \"It is morphing women\\'s faces into porn, stealing their identities, coercing sexual expression, and giving them an identity that they did not choose,\" Citron said last month on a podcast from the University of Virginia, where she teaches and writes about privacy, free expression and civil rights at the university\\'s law school.\\n And then there are the scams\\nIn the past month, visitors to YouTube, Facebook and other platforms have seen video ads purporting to show Jennifer Aniston offering a so-good-it\\'s-delusional deal on Apple laptops.\\n'},\n",
       " {'url': 'https://www.nytimes.com/2024/02/05/business/media/taylor-swift-ai-fake-images.html',\n",
       "  'content': 'Artificial Intelligence\\nArtificial Intelligence\\nAdvertisement\\nArtificial Intelligence\\nSupported by\\nFake and Explicit Images of Taylor Swift Started on 4chan, Study Says\\nThe people on 4chan who created the images of Ms. Swift thought of it as a sort of game, the researchers said.\\n The people on 4chan who created the images of the singer did so in a sort of game, the researchers said — a test to see whether they could create lewd (and sometimes violent) images of famous female figures.\\n By Tiffany Hsu\\nImages of Taylor Swift that had been generated by artificial intelligence and had spread widely across social media in late January probably originated as part of a recurring challenge on one of the internet’s most notorious message boards, according to a new report.\\n Graphika, a research firm that studies disinformation, traced the images back to one community on 4chan, a message board known for sharing hate speech, conspiracy theories and, increasingly, racist and offensive content created using A.I.\\n If you are in Reader mode please exit and\\xa0log into\\xa0your Times account, or\\xa0subscribe\\xa0for all of The Times.\\n'},\n",
       " {'url': 'https://www.cbsnews.com/news/taylor-swift-artificial-intellignence-ai-4chan/',\n",
       "  'content': 'Fake explicit Taylor Swift images raise new concerns about AI 04:07. The fake images of Taylor Swift that spread like wildfire on social media in late January likely began as a chatroom challenge ...'},\n",
       " {'url': 'https://www.cnn.com/2024/01/25/tech/taylor-swift-ai-generated-images/index.html',\n",
       "  'content': \"New York CNN — Pornographic, AI-generated images of the world's most famous star spread across social media this week, underscoring the damaging potential posed by mainstream artificial...\"},\n",
       " {'url': 'https://arstechnica.com/tech-policy/2024/02/4chan-daily-challenge-sparked-deluge-of-explicit-ai-taylor-swift-images/',\n",
       "  'content': '4chan users who have made a game out of exploiting popular AI image generators appear to be at least partly responsible for the flood of fake images sexualizing Taylor Swift that went viral last ...'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tool = TavilySearchResults()\n",
    "tool.invoke({\"query\": \"taylor swift ai pic\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using llm-commons with fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it with langchain\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llm_commons.langchain.proxy import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-4')\n",
    "template = \"Question: {question}\\nAnswer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the world's best B2B company?\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The term \"best\" is subjective and can vary based on different criteria such as revenue, customer satisfaction, innovation, market share, etc. However, some of the world\\'s most successful B2B companies include Microsoft, IBM, and Alibaba. Microsoft is a leading player in providing software, hardware, and services to businesses of all sizes. IBM is known for its IT services and consulting. Alibaba\\'s B2B platform connects suppliers with buyers worldwide. All these companies have made significant impacts in the B2B sector.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "llm_chain.run(\"What is the world's best B2B company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuckDuckGoSearchException",
     "evalue": "_aget_url() https://duckduckgo.com RequestsError: Failed to perform, ErrCode: 28, Reason: 'Failed to connect to duckduckgo.com port 443 after 21240 ms: Couldn't connect to server'. This may be a libcurl error, See https://curl.se/libcurl/c/libcurl-errors.html first for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestsError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:51\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_url\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asession\u001b[38;5;241m.\u001b[39mrequest(method, url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m     resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\curl_cffi\\requests\\session.py:1056\u001b[0m, in \u001b[0;36mAsyncSession.request\u001b[1;34m(self, method, url, params, data, json, headers, cookies, files, auth, timeout, allow_redirects, max_redirects, proxies, proxy, proxy_auth, verify, referer, accept_encoding, content_callback, impersonate, default_headers, http_version, interface, cert, stream, max_recv_speed, multipart)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease_curl(curl)\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m first_element\n\u001b[0;32m   1058\u001b[0m rsp\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m req\n",
      "\u001b[1;31mRequestsError\u001b[0m: Failed to perform, ErrCode: 28, Reason: 'Failed to connect to duckduckgo.com port 443 after 21240 ms: Couldn't connect to server'. This may be a libcurl error, See https://curl.se/libcurl/c/libcurl-errors.html first for more details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DuckDuckGoSearchResults\n\u001b[0;32m      2\u001b[0m search \u001b[38;5;241m=\u001b[39m DuckDuckGoSearchResults()\n\u001b[1;32m----> 4\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat day is it today?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\langchain_core\\tools.py:373\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    372\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    377\u001b[0m     )\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\langchain_core\\tools.py:345\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    343\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    344\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 345\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\langchain_community\\tools\\ddg_search\\tool.py:64\u001b[0m, in \u001b[0;36mDuckDuckGoSearchResults._run\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     60\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     61\u001b[0m     run_manager: Optional[CallbackManagerForToolRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     res_strs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m d\u001b[38;5;241m.\u001b[39mitems()]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m res_strs])\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:111\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.results\u001b[1;34m(self, query, max_results, source)\u001b[0m\n\u001b[0;32m    107\u001b[0m source \u001b[38;5;241m=\u001b[39m source \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    109\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    110\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     ]\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    114\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    115\u001b[0m         {\n\u001b[0;32m    116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddgs_news(query, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m    123\u001b[0m     ]\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:57\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[1;34m(self, query, max_results)\u001b[0m\n\u001b[0;32m     48\u001b[0m     ddgs_gen \u001b[38;5;241m=\u001b[39m ddgs\u001b[38;5;241m.\u001b[39mtext(\n\u001b[0;32m     49\u001b[0m         query,\n\u001b[0;32m     50\u001b[0m         region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m         backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend,\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[1;32m---> 57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mddgs_gen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:27\u001b[0m, in \u001b[0;36mDDGS._iter_over_async\u001b[1;34m(self, async_gen)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__anext__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:101\u001b[0m, in \u001b[0;36mAsyncDDGS.text\u001b[1;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlite\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     99\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_lite(keywords, region, timelimit, max_results)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m result\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:127\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api\u001b[1;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m keywords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords is mandatory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 127\u001b[0m vqd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_vqd(keywords)\n\u001b[0;32m    129\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: region,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    139\u001b[0m }\n\u001b[0;32m    140\u001b[0m safesearch \u001b[38;5;241m=\u001b[39m safesearch\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:64\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_vqd\u001b[1;34m(self, keywords)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aget_vqd\u001b[39m(\u001b[38;5;28mself\u001b[39m, keywords: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get vqd value for a search query.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://duckduckgo.com\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords})\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resp_content:\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _extract_vqd(resp_content, keywords)\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:60\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_url\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp_content\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_aget_url() \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ex)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m: _aget_url() https://duckduckgo.com RequestsError: Failed to perform, ErrCode: 28, Reason: 'Failed to connect to duckduckgo.com port 443 after 21240 ms: Couldn't connect to server'. This may be a libcurl error, See https://curl.se/libcurl/c/libcurl-errors.html first for more details."
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search.run(\"what day is it today?\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "de4a507b2430954be05ae5f9be35829c657a2682621712b642b28df909ac005f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
