{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sap-llm-commons[all] in c:\\python\\3.11\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (8.1.7)\n",
      "Requirement already satisfied: text-generation>=0.6.0 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (0.6.1)\n",
      "Requirement already satisfied: omegaconf in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (2.3.0)\n",
      "Requirement already satisfied: aleph-alpha-client in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (7.0.0)\n",
      "Requirement already satisfied: ai-core-sdk in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (1.22.3)\n",
      "Requirement already satisfied: langchain<=0.0.335,>=0.0.331 in c:\\python\\3.11\\lib\\site-packages (from langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.0.335)\n",
      "Requirement already satisfied: cohere>=4.27 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (4.45)\n",
      "Requirement already satisfied: openai<1 in c:\\python\\3.11\\lib\\site-packages (from sap-llm-commons[all]) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\python\\3.11\\lib\\site-packages (from click>=8.1.3->sap-llm-commons[all]) (0.4.6)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (3.9.1)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (1.9.3)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (6.11.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.0 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python\\3.11\\lib\\site-packages (from cohere>=4.27->sap-llm-commons[all]) (2.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.0.25)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.0.84)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python\\3.11\\lib\\site-packages (from langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (8.2.3)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.3.2 in c:\\python\\3.11\\lib\\site-packages (from langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\python\\3.11\\lib\\site-packages (from openai<1->sap-llm-commons[all]) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in c:\\python\\3.11\\lib\\site-packages (from text-generation>=0.6.0->sap-llm-commons[all]) (0.20.3)\n",
      "Requirement already satisfied: ai-api-client-sdk==1.28.0 in c:\\python\\3.11\\lib\\site-packages (from ai-core-sdk->sap-llm-commons[all]) (1.28.0)\n",
      "Requirement already satisfied: aenum~=3.1 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk==1.28.0->ai-core-sdk->sap-llm-commons[all]) (3.1.15)\n",
      "Requirement already satisfied: pyhumps~=3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk==1.28.0->ai-core-sdk->sap-llm-commons[all]) (3.8.0)\n",
      "Requirement already satisfied: aiodns>=3.0.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (3.1.1)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (2.8.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (4.6.2)\n",
      "Requirement already satisfied: Pillow>=9.2.0 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (9.5.0)\n",
      "Requirement already satisfied: python-liquid>=1.9.4 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (1.10.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\python\\3.11\\lib\\site-packages (from aleph-alpha-client->sap-llm-commons[all]) (23.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\python\\3.11\\lib\\site-packages (from omegaconf->sap-llm-commons[all]) (4.9.3)\n",
      "Requirement already satisfied: pycares>=4.0.0 in c:\\python\\3.11\\lib\\site-packages (from aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\3.11\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere>=4.27->sap-llm-commons[all]) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python\\3.11\\lib\\site-packages (from anyio<4.0->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python\\3.11\\lib\\site-packages (from anyio<4.0->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\3.11\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\3.11\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\3.11\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->text-generation>=0.6.0->sap-llm-commons[all]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\3.11\\lib\\site-packages (from huggingface-hub<1.0,>=0.12->text-generation>=0.6.0->sap-llm-commons[all]) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python\\3.11\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere>=4.27->sap-llm-commons[all]) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\3.11\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python\\3.11\\lib\\site-packages (from pydantic<3,>=1->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\python\\3.11\\lib\\site-packages (from pydantic<3,>=1->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python\\3.11\\lib\\site-packages (from python-liquid>=1.9.4->aleph-alpha-client->sap-llm-commons[all]) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere>=4.27->sap-llm-commons[all]) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere>=4.27->sap-llm-commons[all]) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\3.11\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python\\3.11\\lib\\site-packages (from tiktoken<0.6.0,>=0.3.2->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (2023.12.25)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\python\\3.11\\lib\\site-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\3.11\\lib\\site-packages (from python-dateutil>=2.8.1->python-liquid>=1.9.4->aleph-alpha-client->sap-llm-commons[all]) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\3.11\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<=0.0.335,>=0.0.331->langchain[openai]<=0.0.335,>=0.0.331; extra == \"all\"->sap-llm-commons[all]) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\python\\3.11\\lib\\site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client->sap-llm-commons[all]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sap-llm-commons[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai-api-client-sdk in c:\\python\\3.11\\lib\\site-packages (1.28.0)\n",
      "Requirement already satisfied: aenum~=3.1 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (3.1.15)\n",
      "Requirement already satisfied: pyhumps~=3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0 in c:\\python\\3.11\\lib\\site-packages (from ai-api-client-sdk) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\3.11\\lib\\site-packages (from requests<3.0->ai-api-client-sdk) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install ai-api-client-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylin\\AppData\\Local\\Temp\\ipykernel_3840\\3646097473.py:5: UserWarning: Starting from verison 1.0.0 the default proxy_version was set to 'gen-ai-hub'. Use gen_ai_hub.proxy.core.proxy_clients.set_proxy_version('btp') to set the proxy_version to 'btp'.\n",
      "  import llm_commons.proxy.base\n"
     ]
    }
   ],
   "source": [
    "# proxy configuration\n",
    "from ipywidgets import widgets\n",
    "import json\n",
    "import os\n",
    "import llm_commons.proxy.base\n",
    "\n",
    "llm_commons.proxy.base.proxy_version = 'aicore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_commons.proxy.openai import Completion\n",
    "from llm_commons.proxy.identity import AICoreProxyClient\n",
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "from llm_commons.btp_llm.identity import BTPProxyClient\n",
    "from llm_commons.langchain.proxy import init_llm, init_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5d596f811f44b9ab0edc71c6419f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='default', placeholder='Resource group of deployments')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_group = widgets.Text(\n",
    "    value='default', # resource group\n",
    "    placeholder='Resource group of deployments',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "resource_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/irpa-d26-genaixl-cx-sec-cn-sk.json') as f:\n",
    "    sk = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AICORE_LLM_AUTH_URL'] = sk['url']+\"/oauth/token\"\n",
    "os.environ['AICORE_LLM_CLIENT_ID'] = sk['clientid']\n",
    "os.environ['AICORE_LLM_CLIENT_SECRET'] = sk['clientsecret']\n",
    "os.environ['AICORE_LLM_API_BASE'] = sk[\"serviceurls\"][\"AI_API_URL\"]+ \"/v2\"\n",
    "os.environ['AICORE_LLM_RESOURCE_GROUP'] = resource_group.value\n",
    "os.environ['LLM_COMMONS_PROXY'] = 'aicore'\n",
    "\n",
    "llm_commons.proxy.resource_group = os.environ['AICORE_LLM_RESOURCE_GROUP']\n",
    "llm_commons.proxy.api_base = os.environ['AICORE_LLM_API_BASE']\n",
    "llm_commons.proxy.auth_url = os.environ['AICORE_LLM_AUTH_URL']\n",
    "llm_commons.proxy.client_id = os.environ['AICORE_LLM_CLIENT_ID']\n",
    "llm_commons.proxy.client_secret = os.environ['AICORE_LLM_CLIENT_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_proxy_client = AICoreProxyClient()\n",
    "btp_proxy_client = BTPProxyClient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy customized fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AICoreProxyClient.add_foundation_model_scenario(\n",
    "    scenario_id='fine-tuned-llm',\n",
    "    config_names='fine-tuned-*',\n",
    "    prediction_url_suffix='/v1/completions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dc8e3c266c8881f0', config_id='58a5ffea-1afe-44b4-b6b9-ca749776e58e', config_name='tiiuae--falcon-40b-instruct-config', deployment_id='dc8e3c266c8881f0', model_name='tiiuae--falcon-40b-instruct', created_at=datetime.datetime(2024, 1, 25, 12, 53, 26, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'aicore-opensource', 'model_version': 'null'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dd88f66091ca1982', config_id='a99e09fb-7e88-4a8d-abe8-af7f8b146cb8', config_name='text-embedding-ada-002-config', deployment_id='dd88f66091ca1982', model_name='text-embedding-ada-002', created_at=datetime.datetime(2024, 1, 25, 12, 16, 20, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '2'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d9c8f17b7ee211ac', config_id='015f4d90-32b4-42a6-8a29-d1e919e2814f', config_name='gpt-4-32k-config', deployment_id='d9c8f17b7ee211ac', model_name='gpt-4-32k', created_at=datetime.datetime(2024, 1, 25, 12, 15, 48, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '0613'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d58f5bb41a9941cf', config_id='c5366699-82d2-4dd1-81fb-fa40e187e4bd', config_name='gpt-4-config', deployment_id='d58f5bb41a9941cf', model_name='gpt-4', created_at=datetime.datetime(2024, 1, 25, 12, 15, 2, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': '0613'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d38094e51ff45aff', config_id='c4a9f175-13fb-4526-b7c8-5649f5e7d2fe', config_name='gpt-35-turbo-16k-config', deployment_id='d38094e51ff45aff', model_name='gpt-35-turbo-16k', created_at=datetime.datetime(2024, 1, 25, 12, 14, 18, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': 'latest'}, custom_prediction_suffix=None),\n",
       " Deployment(url='https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d10917558b77a3db', config_id='076308f4-6497-49aa-99ac-9c216b53be74', config_name='gpt-35-turbo-config', deployment_id='d10917558b77a3db', model_name='gpt-35-turbo', created_at=datetime.datetime(2024, 1, 25, 12, 13, 43, tzinfo=datetime.timezone.utc), additonal_parameters={'executable_id': 'azure-openai', 'model_version': 'latest'}, custom_prediction_suffix=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic_proxy_client.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_davinci_003 = Completion(deployment_id='text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'deployment_id' and 'url' have to be of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext_davinci_003\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mour-awesome-model/v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSan Francisco is a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\3.11\\Lib\\site-packages\\llm_commons\\proxy\\openai\\proxy_completion.py:89\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, api_base, auth_url, resource_group, client_id, client_secret, proxy_client, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m     71\u001b[0m            \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m            proxy_client: Optional[ProxyClient] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     78\u001b[0m            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a completion.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    :param api_base: URL of the BTP OpenAI Proxy service\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    :param token: BTP OpenAI Proxy token. Provide either token or auth_url, client_id and client_secret\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    :return: The completion response\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mauth_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mclient_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\3.11\\Lib\\site-packages\\llm_commons\\proxy\\openai\\proxy_completion.py:164\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(cls, api_base, auth_url, resource_group, client_id, client_secret, proxy_client, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create\u001b[39m(\u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    157\u001b[0m             api_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m             proxy_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    163\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 164\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_create_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mauth_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mclient_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ProxyEngineAPIResource\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python\\3.11\\Lib\\site-packages\\llm_commons\\proxy\\openai\\proxy_completion.py:141\u001b[0m, in \u001b[0;36m_create_kwargs\u001b[1;34m(api_base, auth_url, resource_group, client_id, client_secret, proxy_client, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m BaseDeployment\u001b[38;5;241m.\u001b[39mmodel_identification_kwargs:\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 141\u001b[0m deployment \u001b[38;5;241m=\u001b[39m \u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_identification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeployment_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m deployment\u001b[38;5;241m.\u001b[39mdeployment_id\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\python\\3.11\\Lib\\site-packages\\llm_commons\\btp_llm\\identity.py:176\u001b[0m, in \u001b[0;36mBTPProxyClient.get_deployment\u001b[1;34m(self, deployment_id, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using the BTP proxy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeployment_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the only parameter to retrieve a deployment.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    174\u001b[0m          \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m          stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBTPDeployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:5\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, url, deployment_id)\u001b[0m\n",
      "File \u001b[1;32mc:\\python\\3.11\\Lib\\site-packages\\llm_commons\\btp_llm\\identity.py:107\u001b[0m, in \u001b[0;36mBTPDeployment.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment_id, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeployment_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m have to be of type str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'deployment_id' and 'url' have to be of type str"
     ]
    }
   ],
   "source": [
    "#text_davinci_003.create(model_name='our-awesome-model/v1', prompt=\"San Francisco is a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize harmonized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm('gpt-35-turbo', proxy_client = aic_proxy_client,temperature=0., max_tokens=256, deployment_id='d10917558b77a3db', api_base=llm_commons.proxy.api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\python3.12\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'发财树叶子发黄可能是由于以下原因：\\n\\n1. 过浇水：发财树喜欢湿润但排水良好的土壤，如果土壤过于湿润，根部可能会受到损害，导致叶子发黄。\\n\\n2. 缺水：发财树叶子发黄也可能是因为缺水，如果土壤过于干燥，植物无法吸收足够的水分和养分，导致叶子变黄。\\n\\n3. 光照不足：发财树需要充足的阳光才能进行光合作用，如果光照不足，叶子可能会变黄。\\n\\n4. 营养不良：如果土壤中缺乏必要的养分，如氮、磷、钾等，发财树的叶子也会'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"发财树叶子发黄是为什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Model 'text-embedding-ada-002-config' is not available!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43minit_embedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002-config\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maic_proxy_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdd88f66091ca1982\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_commons\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:225\u001b[0m, in \u001b[0;36minit_embedding_model\u001b[1;34m(proxy_client, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_embedding_model\u001b[39m(\u001b[38;5;241m*\u001b[39margs, proxy_client: Optional[BaseProxyClient] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Initializes an embedding model using the specified parameters.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    :rtype: Embeddings\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMBEDDINGS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:172\u001b[0m, in \u001b[0;36m_init_model\u001b[1;34m(proxy_client, model_type, args, kwargs, model_kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_model\u001b[39m(proxy_client: Optional[BaseProxyClient],\n\u001b[0;32m    167\u001b[0m                 model_type: ModelType,\n\u001b[0;32m    168\u001b[0m                 args: List[Any],\n\u001b[0;32m    169\u001b[0m                 kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    170\u001b[0m                 model_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    171\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 172\u001b[0m     retrieval_result: RetrievalResult \u001b[38;5;241m=\u001b[39m \u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieval_result\u001b[38;5;241m.\u001b[39mregistry_entry\u001b[38;5;241m.\u001b[39minit_func(proxy_client\u001b[38;5;241m=\u001b[39mretrieval_result\u001b[38;5;241m.\u001b[39mproxy_client,\n\u001b[0;32m    177\u001b[0m                                                      deployment\u001b[38;5;241m=\u001b[39mretrieval_result\u001b[38;5;241m.\u001b[39mdeployment,\n\u001b[0;32m    178\u001b[0m                                                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:122\u001b[0m, in \u001b[0;36mCatalog.retrieve\u001b[1;34m(self, proxy_client, args, kwargs, model_type)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo model identification argument provided\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m registry_entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_registry_entry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m model_identification_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{n: kwargs[n]\n\u001b[0;32m    130\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m kwarg_names \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m kwargs},\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_identification_kwargs\n\u001b[0;32m    132\u001b[0m }\n\u001b[0;32m    133\u001b[0m f_select_deployment \u001b[38;5;241m=\u001b[39m registry_entry\u001b[38;5;241m.\u001b[39mf_select_deployment \u001b[38;5;129;01mor\u001b[39;00m default_f_select_deployment\n",
      "File \u001b[1;32mj:\\python3.12\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:104\u001b[0m, in \u001b[0;36mCatalog._get_registry_entry\u001b[1;34m(self, proxy_client, model_name, only_available, model_type)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not available!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Model 'text-embedding-ada-002-config' is not available!\""
     ]
    }
   ],
   "source": [
    "embedding = init_embedding_model('text-embedding-ada-002', proxy_client=aic_proxy_client, deployment_id='dd88f66091ca1982', api_base=llm_commons.proxy.api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the world's best B2B company?\"\n",
    "query_embedding = embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is a sample document\"]\n",
    "document_embedding = embedding.embed_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Core proxy support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many top-tier B2B companies, and the \"best\" can vary depending on specific criteria such as revenue, customer satisfaction, or innovation. However, some of the world\\'s leading B2B companies include:\\n\\n1. IBM: A multinational technology company that provides hardware, software, cloud-based services, and cognitive computing.\\n\\n2. Microsoft: They offer a wide range of software products, cloud services, and hardware. Their B2B solutions include Azure, Dynamics 365, and Office 365.\\n\\n3. Amazon Web Services (AWS): Amazon\\'s cloud computing platform provides a wide range of services to businesses across the globe.\\n\\n4. Salesforce: This company is a global leader in customer relationship management (CRM) software.\\n\\n5. Siemens: A global conglomerate that offers a variety of services and products, including industrial automation, energy, and healthcare solutions.\\n\\n6. General Electric (GE): They offer products and services in sectors like aviation, power, renewable energy, digital industry, and healthcare.\\n\\n7. Cisco Systems: A worldwide leader in IT and networking, Cisco provides a variety of hardware, software, and service offerings.\\n\\n8. Adobe Systems: Known for their creative and multimedia software products, Adobe also offers digital marketing solutions.\\n\\n9. Oracle: A multinational computer technology corporation that provides database software, cloud engineered systems, and enterprise software products.\\n\\n10. SAP: A German multinational software corporation that provides enterprise software to manage business operations and customer relations.\\n\\nRemember, the best B2B company would depend on your specific needs and requirements.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_commons.proxy.base import set_proxy_version\n",
    "set_proxy_version('aicore') # for an AI Core proxy\n",
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "chat = ChatOpenAI(proxy_model_name='gpt-4')\n",
    "chat.predict(\"What is the world's best B2B company?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proxy client - use proxies in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_commons.langchain.proxy import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from llm_commons.proxy.openai import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"This is a test.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1706845902,\n",
      "  \"id\": \"chatcmpl-8nesYYfWriCKl5ngRAok9SOwLjvsh\",\n",
      "  \"model\": \"gpt-4\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"detected\": false,\n",
      "          \"filtered\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"prompt_index\": 0\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 5,\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 17\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}]\n",
    "\n",
    "print(ChatCompletion.create(proxy_client=aic_proxy_client, model_name='gpt-4', messages=messages, api_base=os.environ['AICORE_LLM_API_BASE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='This is a test.'\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Say this is a test\"),\n",
    "]\n",
    "# btp_llm = ChatOpenAI(proxy_client=btp_proxy_client, deployment_id='gpt-4')\n",
    "# print(btp_llm.invoke(messages))\n",
    "aic_llm = ChatOpenAI(proxy_client=aic_proxy_client, proxy_model_name='gpt-4')\n",
    "print(aic_llm.invoke(messages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using llm-commons with fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it with langchain\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llm_commons.langchain.proxy import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-4')\n",
    "template = \"Question: {question}\\nAnswer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the world's best B2B company?\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There isn\\'t a definitive answer to this question, as the \"best\" B2B company can vary depending on specific criteria such as industry, scale, innovation, customer service, etc. Some of the world\\'s top B2B companies include Microsoft, Salesforce, and Alibaba. However, it is important to consider the specific needs and objectives of your own company when deciding which B2B company is the \"best\" for you to partner with.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "llm_chain.run(\"What is the world's best B2B company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "de4a507b2430954be05ae5f9be35829c657a2682621712b642b28df909ac005f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
