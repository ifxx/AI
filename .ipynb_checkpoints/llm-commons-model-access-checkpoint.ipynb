{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input path to service instance's credential file\n",
    "Please insert the credentials json file path into \"value\". Ensure the credentials json file to your AI Core are downloaded and provided in the folder where this notebook is located.\n",
    "The credential files for each use case resource group is uploaded in the SAP Passvault. You can find the link for your use case here: https://wiki.one.int.sap/wiki/display/IECOE/SAP+AI+Core+Use+Cases\n",
    "Just download the txt file, place it in this folder and change the extension to .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_creds_file = widgets.Text(\n",
    "    value='<path_to_cred_file>', # service credentials file path\n",
    "    placeholder=\"Path to service instance's credential file\",\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "cf_creds_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insert the resource group value. It can be obtained in AI Launchpad under **\"Workspaces\" -> \"AI API Connections\" -> \"Resource Groups\"**\n",
    "Alternatively your resource group will also be listed on the wiki page from above: https://wiki.one.int.sap/wiki/display/IECOE/SAP+AI+Core+Use+Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = widgets.Text(\n",
    "    value='default', # resource group change this to the given resource group\n",
    "    placeholder='Resource group of deployments',\n",
    "    description='',\n",
    "    disabled=False   \n",
    ")\n",
    "\n",
    "resource_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scenario ID (self-hosted models)\n",
    "Please insert the Scenario ID. The Scenario ID can be obtained in AI Launchpad under **\"ML Operations\" -> \"Scenarios\"**:\n",
    "As most use cases will be working with the foundation models provided by AICore you can probably just leave the Scenario ID as is.\n",
    "<img src=\"imgs/scenarioId.png\">\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_id = widgets.Text(\n",
    "    value='foundation-models', # scenario ID\n",
    "    placeholder='Scenario ID',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "scenario_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Config Names (self-hosted models)\n",
    "Please insert the Configuration Names value. The Configuration Names can be obtained in AI Launchpad under **\"ML Operations\" -> \"Configurations\".**\n",
    "\n",
    "\n",
    "To add multiple configuration names use the prefix with **\\*** in the end (e.g. **\"command-*\"** would include the configurations **\"command-config\"** and **\"command-nightly-config\"**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = widgets.Text(\n",
    "    value='fine-tuned-*', # config names value\n",
    "    placeholder='Configuration names',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "config_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set prediction URL suffix (self-hosted models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_url_suffix = widgets.Text(\n",
    "    value='/v1/chat/completions',\n",
    "    placeholder='',\n",
    "    description='',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "prediction_url_suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment variables from service instance credentials file\n",
    "Execute this cell to set the necessary env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cf_creds_file.value) as cf:\n",
    "    credCF = json.load(cf)\n",
    "\n",
    "base_url=credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\", # The present SAP AI Core API version is 2\n",
    "auth_url= credCF[\"url\"] + \"/oauth/token\", # Suffix to add\n",
    "client_id=credCF['clientid'],\n",
    "client_secret=credCF['clientsecret']\n",
    "\n",
    "os.environ['AICORE_LLM_AUTH_URL'] = auth_url[0]\n",
    "os.environ['AICORE_LLM_CLIENT_ID'] = client_id[0]\n",
    "os.environ['AICORE_LLM_CLIENT_SECRET'] = client_secret\n",
    "os.environ['AICORE_LLM_API_BASE'] = base_url[0]\n",
    "os.environ['AICORE_LLM_RESOURCE_GROUP'] = resource_group.value\n",
    "os.environ['LLM_COMMONS_PROXY'] = 'aicore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import llm-commons libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_commons.proxy.identity import AICoreProxyClient\n",
    "from llm_commons.langchain.proxy import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AI Core Proxy client\n",
    "Execute this cell to create the AI Core Proxy client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_proxy_client = AICoreProxyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_proxy_client.get_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the experimental deployments known to llm-commons (self-hosted models)\n",
    "Execute this cell to add self-hosted models to the AI Core client. \n",
    "\n",
    "Please skip this step if you just want to access the foundation models deployed on AI Core. They should already be visible and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the deployments of experimental models known to llm-commons\n",
    "aic_proxy_client.add_foundation_model_scenario(\n",
    "    scenario_id=scenario_id.value,\n",
    "    config_names=config_names.value,\n",
    "    prediction_url_suffix=prediction_url_suffix.value\n",
    ")\n",
    "\n",
    "# get all deployments including self-hosted models\n",
    "aic_proxy_client.get_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with ChatOpenAI library\n",
    "Use the proxy model name from the above ```.get_deployments()``` response (**\"model_name\"**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(proxy_model_name='gpt-4')\n",
    "chat.predict('What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "chat.predict('What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(proxy_model_name='gpt-35-turbo')\n",
    "chat.predict('What is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(proxy_model_name='gpt-35-turbo-16k')\n",
    "chat.predict('What is your name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with langchain\n",
    "Use the proxy model name from the above ```.get_deployments()``` response (**\"model_name\"**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name='command')\n",
    "template = \"Question: {question}\\nAnswer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "print(llm_chain.run(\"What is the world's best B2B company?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
